# Lab : Ã‰tude de l'Approximation Universelle dans les RÃ©seaux de Neurones

## ğŸ§  Objectif   
L'objectif de ce laboratoire est d'Ã©tudier une propriÃ©tÃ© fondamentale des rÃ©seaux de neurones statiques (non rÃ©currents) : **l'approximation universelle**. Cette propriÃ©tÃ© permet aux rÃ©seaux de neurones, mÃªme avec une seule couche cachÃ©e, d'approximer n'importe quelle fonction continue avec suffisamment de neurones.

Dans ce lab, nous entraÃ®nons un Multi-Layer Perceptron (MLP) avec une seule couche cachÃ©e sur une fonction dÃ©finie par morceaux. Nous analysons ensuite comment le rÃ©seau approxime cette fonction.

## ğŸ“Š Ã‰tapes du Lab
### 1. GÃ©nÃ©ration des DonnÃ©es

- La fonction f(x) est dÃ©finie comme suit :
ğ‘“(ğ‘¥)={
sin(ğœ‹ğ‘¥) siÂ ğ‘¥âˆˆ[âˆ’1,1[
0 siÂ ğ‘¥âˆˆ[âˆ’2,âˆ’1]âˆª[1,2]
- Ajout d'un bruit normal N(0,0.2) aux donnÃ©es gÃ©nÃ©rÃ©es.   
- GÃ©nÃ©ration de multiples Ã©chantillons d'entraÃ®nement et de test Ã  partir de cette fonction.

### 2. ImplÃ©mentation de MLP avec TensorFlow

- Utilisation d'un MLP simple avec une seule couche cachÃ©e pour approximer la fonction gÃ©nÃ©rÃ©e.
- Exploration de diffÃ©rentes architectures avec un nombre de neurones cachÃ©s variant (par exemple, 1, 3, 5, 7 neurones).
  
### 3. Recommandations

- Analyse des rÃ©sultats : DÃ©marrez avec un petit nombre de neurones et observez comment l'approximation de la fonction Ã©volue.
- Variations : Testez diffÃ©rentes configurations de neurones dans la couche cachÃ©e pour observer comment l'approximation devient plus ou moins "sparse" en fonction de la capacitÃ© du modÃ¨le.
- Ã‰valuation des performances : Testez la gÃ©nÃ©ralisation du modÃ¨le en Ã©valuant ses performances sur un ensemble de test.

## ğŸ› ï¸ Technologies UtilisÃ©es  
- `Python` : Langage de programmation principal pour l'implÃ©mentation.   
- `TensorFlow` : Framework pour la crÃ©ation et l'entraÃ®nement du rÃ©seau de neurones MLP.
- `NumPy` : Pour la gÃ©nÃ©ration et la manipulation des donnÃ©es.
- `Matplotlib` : Pour visualiser les approximations des fonctions.
